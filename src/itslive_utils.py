import boto3
import collections
import dask
from dask.diagnostics import ProgressBar
import gc
import itertools
import json
import requests
import pyproj
from pystac_client import Client
import numpy as np
import os
import logging
from rtree import index
import time
import sys
import subprocess
import zarr
import zipfile

from grid import Bounds

BASE_URL = 'https://nsidc.org/apps/itslive-search/velocities/urls'
# BASE_URL = 'https://staging.nsidc.org/apps/itslive-search/velocities/urls'

# Number of 'aws s3 cp' retries in case of a failure
_NUM_AWS_COPY_RETRIES = 20

# Number of seconds to sleep between 'aws s3 cp' retries
_AWS_COPY_SLEEP_SECONDS = 60

# An error generated by AWS when PUT/GET request rate exceeds 3500
_AWS_SLOW_DOWN_ERROR = "An error occurred (SlowDown) when calling"

# Metadata files that exist in the datacube root directory and each
# data variable sub-directory.
CUBE_META = ['.zattrs', '.zgroup', '.zmetadata']
VAR_META = ['.zarray', '.zattrs']

# Extension for the file that contains chunk information
# for each data variable in the datacube
# (ranges for each dimension and last chunk ranges)
# The file is created when the datacube is backed up
# and is used to restore the datacube from the backup.
# The file is created in the same directory as the datacube
# and is named <datacube_name>.chunks.json.
CHUNKS_FILE_EXTENSION = '.chunks.json'


def timing_decorator(func):
    """Decorator to time function execution.

    Args:
        func: Function to invoke.
    """
    def wrapper(*args, **kwargs):
        # Start the timer
        start_time = time.time()

        # Call the function
        result = func(*args, **kwargs)

        # Stop the timer
        end_time = time.time()

        # Calculate elapsed time
        elapsed_time = end_time - start_time
        logging.info(f"Function {func.__name__}() executed in {elapsed_time:.6f} seconds ({elapsed_time/60:.6f} minutes)")

        return result

    return wrapper


# Collection to record chunk information for each data variable in
# existing Zarr store:
# - ranges: list of ranges for each dimension
# - last_dim_ranges: tuple of chunk indices for the last dimension chunk
#   For example, for 3D chunks, it will be a tuple of ranges for 3 indices with the
#  first index being the last chunk index in the first dimension (mid_date for datacubes).
#  For 1D chunks, it will be one tuple to represent one index range.
ZarrChunk = collections.namedtuple(
    "ZarrChunk",
    ['ranges', 'last_dim_ranges']
)


def to_serializable(obj: dict) -> dict:
    """
    Serialize dictionary of ZarrChunk( variable chunk information
    within datacube) for JSON serialization.
    It converts tuples to lists  since JSON doesn't support tuples.

    Args:
        obj (dict): Dictionary of ZarrChunk objects.
    Returns:
        dict: Dictionary with serialized ZarrChunk objects.
    """
    output = {}
    for each_key, each_value in obj.items():
        output[each_key] = {
            'ranges': [[each[0], each[-1]] for each in each_value.ranges],
            'last_dim_ranges': [[each[0], each[-1]] for each in each_value.last_dim_ranges]
        }

    return output


def bucket_cube_name_from_url(source_url: str) -> str:
    """Extract bucket name and file URL from the given datacube URL.

    Args:
        source_url (str): AWS S3 URL of the datacube in Zarr format.

    Returns:
        str: Tuple of bucket name and file URL.
    """
    # Get rid of 's3://' prefix
    source_url = source_url.replace('s3://', '')

    # Split bucket name and file URL
    bucket_name, file_url = source_url.split('/', 1)

    print(f'bucket_name, file_url: {bucket_name}, {file_url}')

    return bucket_name, file_url


def download_chunk(bucket_name, s3_path, each_chunk, local_path):
    """Helper function to download Zarr chunk from S3.

    Args:
        bucket_name (str): Name of the S3 bucket.
        s3_path (str): Path to the datacube or its variable in S3.
        each_chunk (str): Key of the chunk to download.
        local_path (str): Local path to save the downloaded chunk to.
    """
    # logging.info(f'Downloading {s3_key=} to {local_key}')

    # Initialize S3 bucket to copy files from
    s3_bucket = boto3.resource('s3').Bucket(bucket_name)

    s3_bucket.download_file(
        os.path.join(s3_path, each_chunk),
        os.path.join(local_path, each_chunk)
    )


def backup_chunk(bucket, source_path, filename, target_path):
    """Helper function to copy Zarr chunk stored in S3 bucket
    from one location to another. This is used to create a backup
    of the datacube in S3.

    Args:
        bucket (boto3.Bucket): S3 bucket.
        source_path (str): Path to the datacube or its variable in S3.
        filename (str): Name of the file to copy.
        target_path (str): Target path to copy the chunk to.
    """
    # logging.info(f'Copying {source_path=} {filename=} to {local_key}')
    copy_source = {
        'Bucket': bucket.name,
        'Key': os.path.join(source_path, filename)
    }
    bucket.copy(copy_source, os.path.join(target_path, filename))


@timing_decorator
def identify_datacube_latest_chunks(bucket_url: str):
    """
    Identify metadata files and latest chunks for each of the data
    variables for the given datacube s3 URL.

    Args:
        bucket_url (str): Name of the S3 bucket and full path to the
            datacube in Zarr format. Must start with 's3://'.

    Returns:
        Map of data variable to the ranges for existing data chunks,
        and the last chunk ranges for each data variable.
    """
    store = zarr.open_consolidated(
        store=bucket_url,
        mode='r'
    )

    # Identify last chunk for each data variable in the zarr store
    last_chunk_map = {}

    for var_name in store:
        var_obj = store[var_name]
        shape = np.array(var_obj.shape)
        chunk_shape = np.array(var_obj.chunks)

        # Compute total number of chunks along each axis: zero-based indexing
        num_chunks = (shape + chunk_shape - 1) // chunk_shape - 1
        logging.info(f'{var_name=} got {num_chunks=}')

        if len(num_chunks) == 0:
            # For variables with no chunking, just set last chunk to '0'
            # as it exists
            last_chunk_map[var_name] = ZarrChunk([range(0, 1)], [range(0, 1)])
            logging.info(
                f'No chunking for {var_name=}, setting last chunk to {last_chunk_map[var_name]}'
            )

        else:
            # Generate all prior indices per dimension
            dim_ranges = [range(0, idx + 1) for idx in num_chunks]

            # last_chunk_key = ".".join(map(str, num_chunks))
            logging.info(f'{var_name=} {dim_ranges=}')

            # Keep only last chunk for the mid_date dimension
            last_mid_date = num_chunks[0]
            last_chunks = [range(last_mid_date, last_mid_date+1)]

            # There are only 1D, 2D or 3D variables, nothing to do for 1D variables
            if len(num_chunks) == 2:
                # For 2D data variables, have to download all chunks since
                # it corresponds to the whole x/y spatial coverage
                last_chunks = dim_ranges

            elif len(num_chunks) == 3:
                # For 3D data variables, have to download all chunks that correspond
                # to the last chunk in first dimension (mid_date for datacubes)

                # Get list of all chunks that correspond to the last mid_date
                # dimension
                last_chunks.extend(
                    [
                        dim_ranges[1],
                        dim_ranges[2]
                    ]
                )

            logging.info(f'{var_name=}: {dim_ranges=} {last_chunks=}')

            last_chunk_map[var_name] = ZarrChunk(
                dim_ranges,
                last_chunks
            )

    return last_chunk_map


@timing_decorator
def backup_datacube_latest_chunks(
    bucket_url: str,
    backup_path: str,
    num_threads: int = 4,
    dask_scheduler: str = 'threads'
):
    """
    Create backup of metadata files and latest chunks for each of the data
    variables from the given datacube s3 URL.

    Args:
        bucket_url (str): Name of the S3 bucket and full path to the
            datacube in Zarr format.
        backup_path (str): s3 bucket path for the datacube to backup latest
            chunks of the datacube to.
        num_threads (int): Number of threads to use for the backup copy.
            Default is 4.
        dask_scheduler (str): Dask scheduler to use for parallel downloads.
            Default is 'threads'.

    Returns:
        Map of data variable to the ranges for existing data chunks,
        and the last chunk ranges for each data variable.
    """
    # Identify last chunk for each data variable in the zarr store
    last_chunk_map = identify_datacube_latest_chunks(bucket_url)

    bucket_name, source_url = bucket_cube_name_from_url(bucket_url)

    # Save identified chunks to the local file
    local_filename = os.path.basename(backup_path) + CHUNKS_FILE_EXTENSION
    with open(local_filename, 'w') as outfile:
        json.dump(
            to_serializable(last_chunk_map),
            outfile,
            indent=3
        )

    # Get rid of 's3://' prefix and bucket name if present in the source URL
    s3_path = source_url.replace('s3://' + bucket_name + '/', '')

    s3 = boto3.resource('s3')
    s3_bucket = s3.Bucket(bucket_name)

    # Upload chunk information file to the backup path
    s3_bucket.upload_file(local_filename, backup_path + CHUNKS_FILE_EXTENSION)

    # Backup metadata files
    for each_meta in CUBE_META:
        # Backup the file
        logging.info(f'Backup {os.path.join(s3_path, each_meta)} to {backup_path}')
        backup_chunk(s3_bucket, s3_path, each_meta, backup_path)

    # Number of chunks to backup in parallel - should make it configurable?
    num_chunks_in_parallel = 500

    # Backup latest chunks and metadata files for each data variable
    for each_var, each_chunk_info in last_chunk_map.items():
        s3_var_path = os.path.join(s3_path, each_var)
        backup_var_path = os.path.join(backup_path, each_var)

        logging.info(f'Backup {each_var}: {each_chunk_info.last_dim_ranges=}')

        # Step through Cartesian values of the last dimension ranges
        chunk_iterator = itertools.product(*each_chunk_info.last_dim_ranges)

        for chunks in iter(
            lambda: list(
                itertools.islice(
                    chunk_iterator,
                    num_chunks_in_parallel)
                ),
            []
        ):
            tasks = [dask.delayed(backup_chunk)(
                s3_bucket,
                s3_var_path,
                ".".join(map(str, each_chunk)),
                backup_path,
            ) for each_chunk in chunks]

            with ProgressBar():
                _ = dask.compute(
                    tasks,
                    scheduler=dask_scheduler,
                    num_workers=num_threads
                )

            logging.info(
                f'Completed backup {each_var} {len(chunks)} chunks: '
                f'{chunks[0]=} to {chunks[-1]=}'
            )

            del tasks
            gc.collect()

        # Copy variable metadata files
        for each_meta in VAR_META:
            # Download the file
            logging.info(f'Backup {each_meta=} to {backup_path}')
            backup_chunk(
                s3_bucket,
                s3_var_path,
                each_meta,
                backup_var_path
            )

        # Debugger
        # break

    return last_chunk_map


@timing_decorator
def download_datacube_latest_chunks(
    bucket_url: str,
    local_path: str,
    num_threads: int = 4,
    dask_scheduler: str = 'threads'
):
    """
    Download metadata files and latest chunks for each of the data
    variables from the given datacube s3 URL.

    Args:
        bucket_url (str): Name of the S3 bucket and full path to the
            datacube in Zarr format.
        local_path (str): Local directory to save the downloaded files to
            (should be a name of the datacube).
        num_threads (int): Number of threads to use for downloading.
        dask_scheduler (str): Dask scheduler to use for parallel downloads.
                            Default is 'threads'.

    Returns:
        Map of data variable to the ranges for existing data chunks,
        and the last chunk ranges for each data variable.
    """
    # Identify last chunk for each data variable in the zarr store
    last_chunk_map = identify_datacube_latest_chunks(bucket_url)

    bucket_name, source_url = bucket_cube_name_from_url(bucket_url)

    # Get rid of 's3://' prefix and bucket name if present in the source URL
    s3_path = source_url.replace('s3://' + bucket_name + '/', '')

    # Create target directory if it doesn't exist
    if not os.path.exists(local_path):
        os.mkdir(local_path)

    # Download metadata files
    for each_meta in CUBE_META:
        # Download the file
        logging.info(f'Downloading {os.path.join(s3_path, each_meta)} to {local_path}')
        download_chunk(bucket_name, s3_path, each_meta, local_path)

    # Number of chunks to download in parallel - should make it configurable?
    num_chunks_in_parallel = 500

    # Download latest chunks and metadata files for each data variable
    for each_var, each_chunk_info in last_chunk_map.items():
        local_var_path = os.path.join(local_path, each_var)
        s3_var_path = os.path.join(s3_path, each_var)

        if not os.path.exists(local_var_path):
            os.mkdir(local_var_path)

        # Copy last chunks for the data variable
        logging.info(f'Downloading {each_var}: {each_chunk_info.last_dim_ranges=}')

        chunk_iterator = itertools.product(*each_chunk_info.last_dim_ranges)

        for chunks in iter(lambda: list(itertools.islice(chunk_iterator, num_chunks_in_parallel)), []):
            tasks = [dask.delayed(download_chunk)(
                bucket_name,
                s3_var_path,
                ".".join(map(str, each_chunk)),
                local_var_path
            ) for each_chunk in chunks]

            with ProgressBar():
                _ = dask.compute(
                    tasks,
                    scheduler=dask_scheduler,
                    num_workers=num_threads
                )

            logging.info(f'Completed downloading {each_var} {len(chunks)} chunks: {chunks[0]=} to {chunks[-1]=}')

            del tasks
            gc.collect()

        # Copy variable metadata files
        for each_meta in VAR_META:
            # Download the file
            logging.info(f'Downloading {each_meta=} to {os.path.join(local_path, each_var)}')
            download_chunk(
                bucket_name,
                s3_var_path,
                each_meta,
                local_var_path
            )

        # Debugger
        # break

    return last_chunk_map


def get_min_lon_lat_max_lon_lat(coordinates: list):
    """
    Compute longitude and latitude extends for provided coordinates list.
    The coordinates are given in [longitude, latitude] order.

    Args:
    coordinates: list of lists - list of coordinates in [longitude, latitude] order.

    Returns: tuple of (min_lon, min_lat, max_lon, max_lat).
    """
    longitudes = [coord[0] for coord in coordinates]
    latitudes = [coord[1] for coord in coordinates]

    min_lon, max_lon = min(longitudes), max(longitudes)
    min_lat, max_lat = min(latitudes), max(latitudes)

    return (min_lon, min_lat, max_lon, max_lat)


def download_rtree_from_s3(s3_bucket, s3_key, local_path: str = None):
    """
    Download R-tree index file from AWS S3 bucket.

    Args:
    - s3_bucket: Name of the S3 bucket.
    - s3_key: Key of the R-tree file in the S3 bucket.
    - local_path: Local path to save the downloaded file. If None is provided,
            the file will be saved in the current directory with the same name as the S3 key.

    Returns: R-tree index object.
    """
    logging.info(f'Reading R-tree index from s3://{s3_bucket}/{s3_key}')

    for each_extension in ['.dat', '.idx']:
        s3_key_with_ext = s3_key + each_extension
        s3_client = boto3.resource('s3')

        local_file = local_path
        if local_file is None:
            local_file = os.path.basename(s3_key_with_ext)

        logging.info(f'Downloading {s3_key_with_ext} from {s3_bucket} to {local_file}')
        s3_client.Bucket(s3_bucket).download_file(s3_key_with_ext, local_file)

    # Open local version of the R-tree index
    return index.Index(os.path.basename(s3_key))


def query_rtree(rtree_idx, query_box, epsg_code, min_percent_valid_pix=1):
    """
    Query the R-tree for all files overlapping with the query bounding box.
    The query returns only files with at least min_percent_valid_pix valid pixels and
    with the same EPSG code as the query box.

    Args:
    - rtree_idx: R-tree index.
    - query_box: Bounding box to query (min_lon, min_lat, max_lon, max_lat)
    - epsg_code: Original EPSG code of the longitude, latitude coordinates in the query box.
    - min_percent_valid_pix: Minimum percentage of valid pixels in the granule to be considered.
        Default is 1%.

    Returns:
    - List of files names whose extents overlap with the query box.
    """
    # Query the R-tree for files that intersect with the query bounding box
    overlapping_files = list(rtree_idx.intersection(query_box, objects=True))

    # Return file names that overlap the query box and have at least 1% valid pixels
    return [item.object[0] for item in overlapping_files if item.object[1] >= min_percent_valid_pix and item.object[2] == epsg_code]


def s3_copy_using_subprocess(command_line: list, env_copy: dict, is_quiet: bool = True):
    """Copy file to/from aws s3 bucket.

    Args:
    command_line (list): List tokens for the command-line to invoke.
    env_copy (dict): Dictionary of environment variables set for the compute environment.
    is_quiet (bool): Flag if using "quiet" mode to reduce output clutter. Default is True.

    Raises:
        RuntimeError: Failure to copy the store if NUM_AWS_COPY_RETRIES attempts failed.
    """
    _quiet_flag = "--quiet"

    if is_quiet and _quiet_flag not in command_line:
        command_line.append(_quiet_flag)

    logging.info(f'aws s3 command: {" ".join(command_line)}')

    file_is_copied = False
    num_retries = 0
    command_return = None

    while not file_is_copied and num_retries < _NUM_AWS_COPY_RETRIES:
        logging.info(f"Attempt #{num_retries+1} to invoke: {' '.join(command_line)}")

        command_return = subprocess.run(
            command_line,
            env=env_copy,
            check=False,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT
        )

        if command_return.returncode != 0:
            # Report the whole stdout stream as one logging message
            logging.warning(f"Failed to invoke: {' '.join(command_line)} with returncode={command_return.returncode}: {command_return.stdout}")

            num_retries += 1
            # If failed due to AWS SlowDown error, retry
            if num_retries < _NUM_AWS_COPY_RETRIES:
                # Possible to have some other types of failures that are not related to AWS SlowDown,
                # retry the copy for any kind of failure
                # and _AWS_SLOW_DOWN_ERROR in command_return.stdout.decode('utf-8'):

                # Sleep if it's not a last attempt to copy
                time.sleep(_AWS_COPY_SLEEP_SECONDS)

            else:
                # Don't retry, trigger an exception
                num_retries = _NUM_AWS_COPY_RETRIES

        else:
            file_is_copied = True

    if not file_is_copied:
        raise RuntimeError(f"Failed to invoke {' '.join(command_line)} with command.returncode={command_return.returncode}")


@timing_decorator
def search_stac_catalog(epsg_code: str,
                        stac_catalog: str,
                        start_date: str,
                        end_date: str,
                        page_size: int = 2000,
                        percent_valid_pixels: int = 1,
                        **kwargs):
    """
    Search STAC catalog for granules. The code is provided by Luis Lopez.

    Args:
        epsg_code (int): EPSG code of the projection to search for.
        stac_catalog (str): URL to the STAC catalog.
        start_date (str): Start date for the search in 'YYYY-MM-DD' format.
        end_date (str): End date for the search in 'YYYY-MM-DD' format.
        page_size (int): Number of items to return per page. Default is 2000.
        percent_valid_pixels (float): Minimum percentage of valid pixels
            in the granule. Default is None, which means no filtering.
        **kwargs: Additional search parameters.
    Returns:
        list: List of granule URLs.
    """
    # Open the STAC catalog
    logging.info(f"Searching STAC catalog: {stac_catalog}")
    catalog = Client.open(stac_catalog)
    search_kwargs = {
        "collections": ["itslive-granules"],
        "limit": page_size,
        "datetime": f"{start_date}/{end_date}",
        **kwargs
    }

    def build_cql2_filter(filters_list):
        if not filters_list:
            return None
        return filters_list[0] if len(filters_list) == 1 else \
            {"op": "and", "args": filters_list}

    # Add more filters and flexibility if needed
    filter_list = [
        {
            "op": ">=",
            "args": [{"property": "percent_valid_pixels"}, percent_valid_pixels]
        },
        {
            "op": "=",
            "args": [{"property": "proj:code"}, f'EPSG:{epsg_code}']
        }
    ]

    filters = build_cql2_filter(filter_list)
    search_kwargs["filter"] = build_cql2_filter(filters)
    search_kwargs["filter_lang"] = "cql2-json"

    logging.info(f'Using STAC search criteria: {search_kwargs=}')
    search = catalog.search(**search_kwargs)

    hrefs = []
    pages_count = 0

    for page in search.pages():
        pages_count += 1
        for item in page:
            # Check if the item has the requested projection
            for asset in item.assets.values():
                if "data" in asset.roles and asset.href.endswith(".nc"):
                    hrefs.append(asset.href)

    logging.info(
        f'STAC catalog query results: {pages_count=} pages, '
        f'{len(hrefs)=} found granules '
    )

    return hrefs


def transform_coord(proj1, proj2, lon, lat):
    """Transform coordinates from proj1 to proj2 (EPSG num)."""
    # Set full EPSG projection strings
    proj1 = pyproj.Proj("+init=EPSG:"+proj1)
    proj2 = pyproj.Proj("+init=EPSG:"+proj2)
    # Convert coordinates
    return pyproj.transform(proj1, proj2, lon, lat)


def get_granule_urls(params):
    # Allow for longer query time from searchAPI: 10 minutes
    resp = requests.get(BASE_URL, params=params, verify=False, timeout=500)
    return resp.json()


def get_granule_urls_compressed(params, total_retries=1, num_seconds=30):
    """
    Request granules URLs with ZIP compression enabled, save the stream to the ZIP file,
    and retrieve JSON information from the archive.

    params: request parameters
    total_retries: number of retries to query searchAPI in a case of exception.
                    Default is 1.
    num_seconds: number of seconds to sleep between retries to query searchAPI.
                    Default is 30 seconds.
    """
    # Format request URL:
    url = f'{BASE_URL}?'
    for each_key, each_value in params.items():
        url += f'{each_key}={each_value}&'

    # Add compression option
    url += 'compressed=true&'

    # Add requested granules version (TODO: should be configurable on startup?)
    url += 'version=2'

    # Get rid of all single quotes if any in URL
    url = url.replace("'", "")

    num_retries = 0
    got_granules = False
    data = []

    # Save response to local file:
    local_path = 'searchAPI_urls.json.zip'

    logging.info(f'Submitting searchAPI request with url={url}')

    while not got_granules and num_retries < total_retries:
        # Get list of granules:
        try:
            logging.info(f"Getting granules from searchAPI: #{num_retries+1} attempt")
            num_retries += 1

            resp = requests.get(url, stream=True, timeout=500)

            logging.info(f'Saving searchAPI response to {local_path}')
            with open(local_path, 'wb') as fh:
                for chunk in resp.iter_content(10240, decode_unicode=False):
                    _ = fh.write(chunk)

            # Unzip the file
            with zipfile.ZipFile(local_path, 'r') as fh:
                zip_json_file = fh.namelist()[0]
                logging.info(f'Extracting {zip_json_file}')

                with fh.open(zip_json_file) as fh_json:
                    data = json.load(fh_json)

                    got_granules = True

        except:
            # If failed due to response truncation or searchAPI not being able to respond
            # (too many requests at the same time?)
            logging.info(f'Got exception: {sys.exc_info()}')
            if num_retries < total_retries:
                # Sleep if it's not last attempt
                logging.info(f'Sleeping between searchAPI attempts for {num_seconds} seconds...')
                time.sleep(num_seconds)

        finally:
            # Clean up local file
            if os.path.exists(local_path):
                # Remove local file
                logging.info(f'Removing {local_path}')
                os.unlink(local_path)

    if not got_granules:
        raise RuntimeError("Failed to get granules from searchAPI.")

    return data


def get_granule_urls_streamed(params, total_retries=1, num_seconds=30):
    """
    Use streamed retrieval of the response from URL request.

    params: request parameters
    total_retries: number of retries to query searchAPI in a case of exception.
                    Default is 1.
    num_seconds: number of seconds to sleep between retries to query searchAPI.
                    Default is 30 seconds.
    """
    token = ']['

    # Format request URL:
    url = f'{BASE_URL}?'
    for each_key, each_value in params.items():
        url += f'{each_key}={each_value}&'

    # Add requested granules version (TODO: should be configurable on startup?)
    url += 'version=2'

    # Get rid of all single quotes if any in URL
    url = url.replace("'", "")

    num_retries = 0
    got_granules = False
    data = []

    # Save response to local file:
    local_path = 'searchAPI_urls.json'

    logging.info(f'Submitting searchAPI request with url={url}')

    while not got_granules and num_retries < total_retries:
        # Get list of granules:
        try:
            logging.info(f"Getting granules from searchAPI: #{num_retries+1} attempt")
            num_retries += 1

            resp = requests.get(url, stream=True, timeout=500)

            logging.info(f'Saving searchAPI response to {local_path}')
            with open(local_path, 'a') as fh:
                for chunk in resp.iter_content(10240, decode_unicode=True):
                    _ = fh.write(chunk)

            # Read data from local file
            data = ''
            with open(local_path) as fh:
                data = fh.readline()

            # if multiple json strings are returned,  then possible to see '][' within
            # the string, replace it by ','
            if token in data:
                logging.info('Got multiple json variables within the same string (len(data)={len(data)})')
                data = data.replace(token, ',')

                logging.info('Merged multiple json variables into one list (len(data)={len(data)})')

            data = json.loads(data)
            got_granules = True

        except:
            # If failed due to response truncation or searchAPI not being able to respond
            # (too many requests at the same time?)
            logging.info(f'Got exception: {sys.exc_info()}')
            if num_retries < total_retries:
                # Sleep if it's not last attempt
                logging.info(f'Sleeping between searchAPI attempts for {num_seconds} seconds...')
                time.sleep(num_seconds)

        finally:
            # Clean up local file
            if os.path.exists(local_path):
                # Remove local file
                logging.info(f'Removing {local_path}')
                os.unlink(local_path)

    if not got_granules:
        raise RuntimeError("Failed to get granules from searchAPI.")

    return data


#
# Author: Mark Fahnestock
#
def point_to_prefix(lat: float, lon: float, dir_path: str = None) -> str:
    """
    Returns a string (for example, N78W124) for directory name based on
    granule centerpoint lat,lon
    """
    NShemi_str = 'N' if lat >= 0.0 else 'S'
    EWhemi_str = 'E' if lon >= 0.0 else 'W'

    outlat = int(10*np.trunc(np.abs(lat/10.0)))
    if outlat == 90:  # if you are exactly at a pole, put in lat = 80 bin
        outlat = 80

    outlon = int(10*np.trunc(np.abs(lon/10.0)))

    if outlon >= 180:  # if you are at the dateline, back off to the 170 bin
        outlon = 170

    dirstring = f'{NShemi_str}{outlat:02d}{EWhemi_str}{outlon:03d}'
    if dir_path is not None:
        dirstring = os.path.join(dir_path, dirstring)

    return dirstring


#
# Author: Mark Fahnestock, Masha Liukis
#
def add_five_points_to_polygon_side(polygon):
    """
    Define 5 points per each polygon side. This is done before re-projecting
    polygon to longitude/latitude coordinates.
    This function assumes rectangular polygon where min/max x/y define all
    4 polygon vertices.

    polygon: list of lists
        List of polygon vertices.
    """
    fracs = [0.25, 0.5, 0.75]
    polylist = []  # closed ring of polygon points

    # Determine min/max x/y values for the polygon
    x = Bounds([each[0] for each in polygon])
    y = Bounds([each[1] for each in polygon])

    polylist.append((x.min, y.min))
    dx = x.max - x.min
    dy = y.min - y.min
    for frac in fracs:
        polylist.append((x.min + frac * dx, y.min + frac * dy))

    polylist.append((x.max, y.min))
    dx = x.max - x.max
    dy = y.max - y.min
    for frac in fracs:
        polylist.append((x.max + frac * dx, y.min + frac * dy))

    polylist.append((x.max, y.max))
    dx = x.min - x.max
    dy = y.max - y.max
    for frac in fracs:
        polylist.append((x.max + frac * dx, y.max + frac * dy))

    polylist.append((x.min, y.max))
    dx = x.min - x.min
    dy = y.min - y.max
    for frac in fracs:
        polylist.append((x.min + frac * dx, y.max + frac * dy))

    polylist.append((x.min, y.min))

    return polylist
